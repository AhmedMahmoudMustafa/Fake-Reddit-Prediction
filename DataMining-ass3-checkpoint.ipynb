{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dce8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libiraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline #Pipeline of transforms with a final estimator.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to Convert text to a matrix of TF-IDF features.\n",
    "from sklearn.linear_model import LogisticRegression #importing the logistic regression model\n",
    "from bokeh.models import NumeralTickFormatter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from nltk.stem.snowball import SnowballStemmer#stemmer converts words to its root form\n",
    "from nltk.tokenize import word_tokenize # we will be able to extract the tokens from string of characters\n",
    "from nltk.corpus import stopwords # imported it to be able to remove stop words\n",
    "import nltk #The Natural Language Toolkit which are libraries for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa068c0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  label\n",
       "0  265723  A group of friends began to volunteer at a hom...      0\n",
       "1  284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
       "2  207715  In 1961, Goodyear released a kit that allows P...      0\n",
       "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the training data\n",
    "df = pd.read_csv('xy_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce583be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stargazer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0                                         stargazer \n",
       "1   1                                               yeah\n",
       "2   2  PD: Phoenix car thief gets instructions from Y...\n",
       "3   3  As Trump Accuses Iran, He Has One Problem: His...\n",
       "4   4                       \"Believers\" - Hezbollah 2011"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the testing data\n",
    "df1 = pd.read_csv('x_test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14849a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the data has null values or not\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f00a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      60000 non-null  int64 \n",
      " 1   text    60000 non-null  object\n",
      " 2   label   60000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#info of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92988544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d433d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32172\n",
       "1    27596\n",
       "2      232\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frequency of each class\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa6380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace label 2 with 1\n",
    "df.loc[df['label'] == 2, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374716be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    32172\n",
       "1    27828\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ade6d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5362\n",
       "1    0.4638\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking distribution of target variable values\n",
    "\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21f23e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the duplicates in data\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a233e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ahmed\n",
      "[nltk_data]     Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ahmed\n",
      "[nltk_data]     Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') #downloading the tokenizer which divides a text into a list of sentences from nltk\n",
    "nltk.download('stopwords')#Stop words are words that are going to be ignored by tokenizers.\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") #stemmer converts words to its root form, for english language\n",
    "stop_words = set(stopwords.words(\"english\"))#to be able to remove stop words for english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8324e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, for_embedding=False): #defining a function for cleaning \n",
    "    \"\"\" steps:\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding :\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) #for converting whitespacs to single white space\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\") #for removing html tags\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE) #removing ASCII chars and european chars\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE) # removing single letter chars\n",
    "    if for_embedding: #   if not for embedding :\n",
    "        # - all lowercase\n",
    "        # - remove stopwords, punctuation and stemm\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text) ## to extract the tokens from string of characters\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd12356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df.loc[df['text'].str.len() > 5, 'text'] #defining a text_clean column in df \n",
    "df[\"text_clean\"] = df[\"text\"].map( #applying the text_clean function on the 'text' column and putting the results in the text_clean column \n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e76eaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"text_clean\"] = df1.loc[df1[\"text\"].str.len() > 5, \"text\"] #defining a text clean column in testing set\n",
    "df1[\"text_clean\"] = df1[\"text\"].map(  #applying the text_clean function on the 'text' column and putting the results in the text_clean column\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a9c413a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265723</td>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "      <td>group friend began volunt homeless shelter nei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>284269</td>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "      <td>british prime minist theresa may nerv attack f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207715</td>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "      <td>goodyear releas kit allow ps brought heel http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551106</td>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "      <td>happi birthday bob barker price right host lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8584</td>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "      <td>obama nation innoc cop unarm young black men d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  label  \\\n",
       "0  265723  A group of friends began to volunteer at a hom...      0   \n",
       "1  284269  British Prime Minister @Theresa_May on Nerve A...      0   \n",
       "2  207715  In 1961, Goodyear released a kit that allows P...      0   \n",
       "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0   \n",
       "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  group friend began volunt homeless shelter nei...  \n",
       "1  british prime minist theresa may nerv attack f...  \n",
       "2  goodyear releas kit allow ps brought heel http...  \n",
       "3  happi birthday bob barker price right host lik...  \n",
       "4  obama nation innoc cop unarm young black men d...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bbb1c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stargazer</td>\n",
       "      <td>stargaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>yeah</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
       "      <td>pd phoenix car thief get instruct youtub video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
       "      <td>trump accus iran one problem credibl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "      <td>believ hezbollah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0                                         stargazer    \n",
       "1   1                                               yeah   \n",
       "2   2  PD: Phoenix car thief gets instructions from Y...   \n",
       "3   3  As Trump Accuses Iran, He Has One Problem: His...   \n",
       "4   4                       \"Believers\" - Hezbollah 2011   \n",
       "\n",
       "                                       text_clean  \n",
       "0                                         stargaz  \n",
       "1                                            yeah  \n",
       "2  pd phoenix car thief get instruct youtub video  \n",
       "3            trump accus iran one problem credibl  \n",
       "4                                believ hezbollah  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61bc196b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year      4139\n",
       "one       3305\n",
       "like      3144\n",
       "new       3010\n",
       "look      2862\n",
       "color     2742\n",
       "man       2737\n",
       "get       2614\n",
       "trump     2602\n",
       "say       2360\n",
       "peopl     2326\n",
       "use       2312\n",
       "first     2257\n",
       "make      2241\n",
       "old       2233\n",
       "time      2038\n",
       "found     2002\n",
       "poster    2002\n",
       "day       1941\n",
       "war       1869\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing Word Frequency of most common words\n",
    "word_freq = pd.Series(\" \".join(df[\"text_clean\"]).split()).value_counts()\n",
    "word_freq[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9e1b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into x and y for training data and x for testing data\n",
    "X=df['text_clean']\n",
    "Y=df['label']\n",
    "X_test=df1['text_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057bee4",
   "metadata": {},
   "source": [
    "### Building the pipeline - TF-idfVectorizer(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5999155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=word as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline #Pipeline of transforms with a final estimator.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to Convert text to a matrix of TF-IDF features.\n",
    "from sklearn.linear_model import LogisticRegression #importing the logistic regression model\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"word\")),\n",
    "                 (\"lr\", LogisticRegression())]) #we will use the word vectorizer and the logistic regrression model\n",
    "\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(1, 4),(2,5)],#n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.6, 0.8),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(5,30),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "    'lr__penalty' : ['l1','l2'], #Penalized logistic regression imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive variables toward zero. \n",
    "    'lr__C' : np.logspace(-3,3,30), #C value controls the strength of the penalty\n",
    "    'lr__solver': [ 'liblinear','newton-cg', 'lbfgs'] #setting solver options/ranges to liblinear,newton-cg,lbfgs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31f7c3",
   "metadata": {},
   "source": [
    "### LR Model using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6822f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.82303289 0.63931586 0.63385889        nan 0.72690006        nan\n",
      "        nan 0.68790684        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.82303289399392\n",
      "best hyperparameter {'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 21, 'tfidf__max_df': 0.6, 'lr__solver': 'lbfgs', 'lr__penalty': 'l2', 'lr__C': 0.0041753189365604}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, train_size = 0.8, stratify = Y, random_state = 45)#the random_state parameter is used for initializing the internal random number generator\n",
    "#The stratify option instructs sklearn to divide the dataset into a test and training set with the ratio of class labels in the variable supplied.\n",
    "\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X_train\n",
    "split_index = [-1 if x in X_train.index else 0 for x in X.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "#\n",
    "random_search_lr = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=-1, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')#The degree of separability/distinction or intermingling/crossover between the forecasts of the two classes is shown by the ROC-AUC.\n",
    "\n",
    "random_search_lr.fit(X,Y)  #fitting my data into random_search_lr results\n",
    "\n",
    "print('best score {}'.format(random_search_lr.best_score_)) #printing best scores\n",
    "print('best hyperparameter {}'.format(random_search_lr.best_params_)) #printing best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97c7ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict output and save submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df1['id']\n",
    "submission['label']=random_search_lr.predict_proba(df1['text'])[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af784259",
   "metadata": {},
   "source": [
    "### XGB Model using TfidfVectorizer(\"char\") RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8f2016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=char as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"char\")), \n",
    "                 ('xgb_model', XGBClassifier())])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(1, 4),(2,5)], # #n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.6, 0.8),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(5,30),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "    'xgb_model__min_child_weight': [1, 5, 10], #The number of samples required to form a leaf node (the end of a branch)\n",
    "    'xgb_model__gamma': [0.5, 1, 1.5, 2, 5],#The gamma is an unbounded parameter from 0 to infinity that is used to control the model’s tendency to overfit.\n",
    "    'xgb_model__subsample': [0.6, 0.8, 1.0], # determines how much of the initial dataset is fair game for random sampling during each iteration of the boosting process.\n",
    "    'xgb_model__colsample_bytree': [0.6, 0.8, 1.0],#defines percentage of features will be used for building each tree.\n",
    "    'xgb_model__max_depth': [3, 4, 5]# how deep each estimator is permitted to build a tree.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1d89a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "best score 0.8172086151643863\n",
      "best hyperparameter {'xgb_model__subsample': 0.8, 'xgb_model__min_child_weight': 1, 'xgb_model__max_depth': 4, 'xgb_model__gamma': 1, 'xgb_model__colsample_bytree': 1.0, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 27, 'tfidf__max_df': 0.6}\n"
     ]
    }
   ],
   "source": [
    "random_search_xg = RandomizedSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=5, \n",
    "    # number of random trials\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search_xg.fit(X,Y) #fitting my x and y into the xgboost random search rresults\n",
    "\n",
    "print('best score {}'.format(random_search_xg.best_score_)) #printing best scores\n",
    "print('best hyperparameter {}'.format(random_search_xg.best_params_)) #printing best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb6481ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict output and save submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df1['id']\n",
    "submission['label']=random_search_xg.predict_proba(df1['text'])[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8ca5c",
   "metadata": {},
   "source": [
    "### ### RandomForest Model using TfidfVectorizer(\"word\") GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cf9d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=word as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(\"word\")),  \n",
    "                 ('my_classifier',RandomForestClassifier())])\n",
    "\n",
    "params = {\n",
    "    \n",
    "    \"tfidf__ngram_range\": [(3, 6),(4,8)],#n-gram range:tuple (min_n, max_n), The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.9),#When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    \"tfidf__min_df\": np.arange(8,24),#When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "    'my_classifier__n_estimators': [50, 100, 150],  # ranges of n_estimators which are the number of trees to be used in the forest.\n",
    "    # I set my  n_estimators ranges to [50, 100, 150]\n",
    "    'my_classifier__max_depth':[30, 60, 90]  \n",
    "    # I set my  max depth ranges to [30, 60, 90] which are The number of splits that each decision tree is allowed to make\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca14bb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 288 candidates, totalling 288 fits\n",
      "best score 0.5259320962757253\n",
      "best hyperparameter {'my_classifier__max_depth': 30, 'my_classifier__n_estimators': 100, 'tfidf__max_df': 0.3, 'tfidf__min_df': 8, 'tfidf__ngram_range': (3, 6)}\n"
     ]
    }
   ],
   "source": [
    "random_search_rnf = GridSearchCV(\n",
    "    pipe, params, cv=pds, verbose=1, n_jobs=-1, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search_rnf.fit(X,Y)\n",
    "\n",
    "print('best score {}'.format(random_search_rnf.best_score_))\n",
    "print('best hyperparameter {}'.format(random_search_rnf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204d437",
   "metadata": {},
   "source": [
    "### LogisticRegression Model using TfidfVectorizer(\"word\") GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3aa78dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.82117534        nan 0.84381273        nan 0.85837461\n",
      "        nan 0.69093019        nan 0.69763399        nan 0.70262876\n",
      "        nan 0.82383922        nan 0.84698256        nan 0.86400585]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9144142059191298\n",
      "Best params: {'cvec__ngram_range': (1, 3), 'lr__C': 1, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Further split the original training set to a train and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, train_size = 0.8, stratify = Y, random_state = 2022)\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "split_index = [-1 if i in X_train.index else 0 for i in X.index]\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "#set vectorizer hyperparameters\n",
    "pipe = Pipeline([('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")),    \n",
    "                 ('lr', LogisticRegression(solver='sag'))])\n",
    "# Tune GridSearchCV\n",
    "pipe_params = {'cvec__ngram_range': [(1,1), (2,2), (1,3)],'lr__C': [0.01, 0.1,1], 'lr__penalty': ['l1', 'l2']}\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params,  scoring=\"roc_auc\", cv=pds)\n",
    "#train model with gridsearchcv\n",
    "gs.fit(X, Y);\n",
    "#predict best score and params\n",
    "print(\"Train score\", gs.score(X, Y))\n",
    "print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed953584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict output and save submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df1['id']\n",
    "submission['label']=gs.predict_proba(df1['text'])[:,1]\n",
    "submission.to_csv('sample_submission_walkthrough2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8198d22",
   "metadata": {},
   "source": [
    "### LogisticRegression Model using TfidfVectorizer(\"word\") GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b3c0cea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan 0.82117593        nan 0.84381309        nan 0.85837489\n",
      "        nan 0.69093231        nan 0.69763402        nan 0.70262887\n",
      "        nan 0.8238407         nan 0.8469827         nan 0.86400568\n",
      "        nan 0.82386461        nan 0.84698569        nan 0.86399834\n",
      "        nan 0.82387611        nan 0.84698804        nan 0.86399745\n",
      "        nan 0.52166156        nan 0.52167303        nan 0.52170015\n",
      "        nan 0.50379546        nan 0.50379548        nan 0.5037961 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.9144144751079306\n",
      "Best params: {'cvec__ngram_range': (1, 3), 'lr__C': 1, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Further split the original training set to a train and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, train_size = 0.8, stratify = Y, random_state = 2022)\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "split_index = [-1 if i in X_train.index else 0 for i in X.index]\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)\n",
    "\n",
    "#set vectorizer hyperparameters\n",
    "pipe = Pipeline([('cvec', TfidfVectorizer(preprocessor=clean_text,analyzer=\"word\", max_df=0.3, min_df=10, norm=\"l2\")),    \n",
    "                 ('lr', LogisticRegression(solver='sag'))])\n",
    "# Tune GridSearchCV\n",
    "pipe_params = {'cvec__ngram_range': [(1,1), (2,2), (1,3),(1, 4), (1, 5),(3, 6), (4, 8)],'lr__C': [0.01, 0.1,1], 'lr__penalty': ['l1', 'l2'], }\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params,  scoring=\"roc_auc\", cv=pds)\n",
    "#train model with gridsearchcv\n",
    "gs.fit(X, Y);\n",
    "#predict best score and params\n",
    "print(\"Train score\", gs.score(X, Y))\n",
    "print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad4b6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict output and save submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = df1['id']\n",
    "submission['label']=gs.predict_proba(df1['text'])[:,1]\n",
    "submission.to_csv('LRGS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c07b8",
   "metadata": {},
   "source": [
    "### 2nd trial LogisticRegression Model using TfidfVectorizer(\"word\") GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97991675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass input=word as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n",
      "C:\\Users\\Ahmed Mahmoud\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.79793287 0.79796923 0.50424463 ... 0.83196527        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature creation and modelling in a single function\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer('word')),\n",
    "                 (\"LR\", LogisticRegression())])\n",
    "\n",
    "# define parameter space to test # runtime \n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 4), (1, 5),(3, 6), (4, 8)],\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
    "    \"tfidf__min_df\": np.arange(50, 100),\n",
    "     'LR__penalty' : ['l1', 'l2'],\n",
    "    'LR__C' : [0.1,0.5],\n",
    "    'LR__solver' : ['liblinear','sag','saga'],\n",
    "    'LR__max_iter': [50,100]\n",
    "}\n",
    "# it is quite slow so we do 4 for now\n",
    "pipe_clf = GridSearchCV(\n",
    "    pipe, params, n_jobs=-1, scoring=\"roc_auc\", cv=pds)\n",
    "pipe_clf.fit(X, Y)\n",
    "pickle.dump(pipe_clf, open(\"./clf_pipe.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccabbc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR__C': 0.5, 'LR__max_iter': 50, 'LR__penalty': 'l2', 'LR__solver': 'saga', 'tfidf__max_df': 0.3, 'tfidf__min_df': 50, 'tfidf__ngram_range': (1, 5)}\n"
     ]
    }
   ],
   "source": [
    "best_params = pipe_clf.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dfeee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      6434\n",
      "           1       0.77      0.79      0.78      5566\n",
      "\n",
      "    accuracy                           0.79     12000\n",
      "   macro avg       0.79      0.79      0.79     12000\n",
      "weighted avg       0.79      0.79      0.79     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run pipe with optimized parameters\n",
    "pipe.set_params(**best_params).fit(X, Y)\n",
    "pipe_pred = pipe.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, pipe_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9864ebf",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "### The Problem is:\n",
    "We have data contains text column and each row include title and each title will be classed as fake or not. the problem is that the data contains contains various forms of words so we should apply text preprocessing techniques ti clean it.\n",
    "\n",
    "### What is the input?\n",
    "Text coulmn includes titles that will be classified as fake or not.\n",
    "\n",
    "### What is the output?\n",
    "Probability of how much is the title classified as fake.\n",
    "\n",
    "### What data mining function is required?\n",
    "Drop duplicate\n",
    "\n",
    "Drop useless rows\n",
    "\n",
    "re.sub\n",
    "\n",
    "re.compile\n",
    "\n",
    "re.IGNORECASE\n",
    "\n",
    "word.lower()\n",
    "\n",
    "value_counts\n",
    "\n",
    "Steeming\n",
    "\n",
    "Stop words\n",
    "\n",
    "### What could be the challenges?\n",
    "To remove operation signs from texts and stop words to make text more clear. To make computer understand human words, In a normal conversation between humans, things are often unsaid, whether in the form of some signal, expression, or just silence. Nevertheless, we, as humans, have the capacity to understand the underlying intent of the conversation, which a computer lacks. A second difficulty is owing to ambiguity in sentences. This may be at the word level, at the sentence level, or at the meaning level.\n",
    "\n",
    "### What is the impact?\n",
    "The model will truly understand human language and classify if news is fake or not from it's title only.\n",
    "\n",
    "### What is an ideal solution?\n",
    "Logistic regression model with word-level victorizer and GridSearchCV.\n",
    "\n",
    "### What is the experimental protocol used and how was it carried out?\n",
    "tf-idf Char level vectorizer and tf-idf Word level vectorizer. Both are good and each one of them is good with sort of data. We cannot decide which is better than other, It depends only on trained data.\n",
    "\n",
    "### What preprocessing steps are used?\n",
    "We dropped duplicate rows and clear data that includes false label.\n",
    "We cleared stop words.\n",
    "We cleared operations signs to make classifying easier.\n",
    "Convert upper cases to lower.\n",
    "Use tf-idf with word vectorizer and char vectorizer.\n",
    "## Questions\n",
    "### 🌈 What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "Character n-gram compute how much charachter repeated depends on the selected number, but word n-gram compute how much word repeated depends on selected number.\n",
    "\n",
    "Character Tokenizers handles OOV words coherently by preserving the information of the word. It breaks down the OOV word into characters and represents the word in terms of these characters. It also limits the size of the vocabulary. since the 26 vocabulary contains a unique set of characters.\n",
    "\n",
    "### 🌈 What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "Stop word removes words that not add much value to the meaning of the text.\n",
    "\n",
    "like (“the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc.).\n",
    "\n",
    "Steeming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word.\n",
    "\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "\n",
    "It is highly dependent on the task we are performing.\n",
    "\n",
    "### 🌈 Is tokenization techniques language dependent? Why?\n",
    "Yes, Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n",
    "\n",
    "### 🌈 What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "Count Vectorizer is a way to convert a given set of strings into a frequency representation.\n",
    "\n",
    "TF-IDF stands for Term Frequency — Inverse Document Frequency and is a statistic that aims to better define how important a word is for a document, while also taking into account the relation to other documents from the same corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e92695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
